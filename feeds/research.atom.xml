<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Michael Saxon - Research</title><link href="https://saxon.me/" rel="alternate"></link><link href="https://saxon.me/feeds/research.atom.xml" rel="self"></link><id>https://saxon.me/</id><updated>2025-10-16T21:05:00-07:00</updated><entry><title>COLM 2025: 9 cool papers and some thoughts</title><link href="https://saxon.me/blog/2025/colm2025/" rel="alternate"></link><published>2025-10-16T21:05:00-07:00</published><updated>2025-10-16T21:05:00-07:00</updated><author><name>Michael Saxon</name></author><id>tag:saxon.me,2025-10-16:/blog/2025/colm2025/</id><summary type="html">Reflections on the 2025 COLM conference, and a discussion of 9 cool COLM papers on benchmarking and eval, personas, and improving models for better long-context performance and consistency.</summary><content type="html">&lt;p&gt;COLM 2025 was a blast! Here I'll summarize takeaways I had from poster discussions for 9 cool papers, and I'll give some of my own reflections on the conference overall.&lt;/p&gt;
&lt;div class="toc"&gt;&lt;span class="toc-title"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#benchmarking-and-evaluation"&gt;Benchmarking and evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#code-lms-struggle-to-generate-good-test-cases"&gt;Code LMs struggle to generate good test cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#base-models-are-more-creative-than-aligned"&gt;Base models are more creative than aligned&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lms-struggle-with-long-range-plot-dependencies"&gt;LMs struggle with long-range plot dependencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#item-response-theory-smooths-benchmarks"&gt;Item response theory smooths benchmarks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#modifying-and-improving-models"&gt;Modifying and improving models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#data-augmentation-for-improving-long-context-models"&gt;Data augmentation for improving long-context models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reducing-miscalibration-in-bidirectional-knowledge"&gt;Reducing miscalibration in bidirectional knowledge&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#personas"&gt;Personas&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#finetuned-lms-can-successfully-impersonate-you-and-trick-your-friends"&gt;Finetuned LMs can successfully impersonate you and trick your friends&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#detailed-life-stories-give-more-diverse-and-distributionally-accurate-simulated-humans"&gt;Detailed life stories give more diverse and distributionally accurate simulated humans&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#personas-for-evaluating-personalization"&gt;Personas for evaluating personalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#roses-and-thorns"&gt;Roses and thorns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h1 id="benchmarking-and-evaluation"&gt;&lt;a class="toclink" href="#benchmarking-and-evaluation"&gt;Benchmarking and evaluation&lt;/a&gt;&lt;a class="header-link" href="#benchmarking-and-evaluation" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;First, a certified eval enjoyer I am obligated to point some cool benchmark and metric papers! Rapid fire:&lt;/p&gt;
&lt;h2 id="code-lms-struggle-to-generate-good-test-cases"&gt;&lt;a class="toclink" href="#code-lms-struggle-to-generate-good-test-cases"&gt;Code LMs struggle to generate good test cases&lt;/a&gt;&lt;a class="header-link" href="#code-lms-struggle-to-generate-good-test-cases" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;a href="https://arxiv.org/abs/2502.19414"&gt;Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation&lt;/a&gt; Shiven Sinha (IIT Hyderabad) et al introduce the REFUTE benchmark for codegen: a neat new eval for the task of writing &lt;em&gt;falsifying test cases&lt;/em&gt; given a pair of (working, non-working) scripts. I submit that if a model really understands code, this should be a trivially easy task, as the model gets to see the function that contains an error! However, when comparing models at generating code for each problem vs writing counterexamples, there were extreme disparities. For example, o3-mini could pass 48% of these codeforces problems, but only write counterexamples (with demonstrations!) to 8.6%.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_008.jpeg"/&gt;&lt;/p&gt;
&lt;h2 id="base-models-are-more-creative-than-aligned"&gt;&lt;a class="toclink" href="#base-models-are-more-creative-than-aligned"&gt;Base models are more creative than aligned&lt;/a&gt;&lt;a class="header-link" href="#base-models-are-more-creative-than-aligned" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Peter West (UBC) demonstrated how &lt;a href="https://arxiv.org/abs/2505.00047"&gt;Base Models Beat Aligned Models at Randomness and Creativity&lt;/a&gt; using random number guessing, mixed-strategy games, and poetry writing (as judged by humans). This has implications for a variety of problems: when the need is to sample diverse examples from a model, it's probably better to go with base.
The poetry examples were particularly evocative to me. When assessing human notions of &lt;em&gt;creativity&lt;/em&gt; vs human ratings of &lt;em&gt;pleasantness&lt;/em&gt;, the example of "write a poem about coffee" illustrates the distinction.&lt;/p&gt;
&lt;p&gt;Llama 3 base 70B gives:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Adenosine wrapped in caffeine,&lt;br/&gt;
Dance through my receptors within. &lt;br/&gt;
Stimulate; neurotransmitters go insane.&lt;br/&gt;
Kicked inside my synapses; wake up brain!  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tulu-full (Llama 3 70B) gives:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rich aroma fills the air&lt;br/&gt;
As morning dawns, with coffee's care&lt;br/&gt;
Warm and dark, a soothing friend&lt;br/&gt;
Lifting fog, till the day's end.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Personally, for the uniqueness (and the machine-ness!) of the first one, I prefer it. But the second one is definitely easier.&lt;/p&gt;
&lt;h2 id="lms-struggle-with-long-range-plot-dependencies"&gt;&lt;a class="toclink" href="#lms-struggle-with-long-range-plot-dependencies"&gt;LMs struggle with long-range plot dependencies&lt;/a&gt;&lt;a class="header-link" href="#lms-struggle-with-long-range-plot-dependencies" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;a href="https://arxiv.org/abs/2504.11900"&gt;Finding Flawed Fictions&lt;/a&gt; Kabir Ahuja et al (UW) introduce plot hole detection as a task for evaluating long-context models. They introduce an algorithm to create plot holes inside of texts, and use this to build the FlawedFictions benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_009.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;I am really happy to see contributions like this---ecologically valid and challenging long-context understanding in LMs is understudied. Despite strong and simple "needle in a haystack" results&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;span class="footnote-popup"&gt;(see &lt;a href="https://techcrunch.com/2024/06/29/geminis-data-analyzing-abilities-arent-as-good-as-google-claims/"&gt;TechCrunch interview&lt;/a&gt; with Marzena Karpinska and myself on this issue) &lt;/span&gt;&lt;/sup&gt; even frontier LMs fail to generalize to basic long-context tasks such as finding easy information in long, unrelated context, or answering simple questions about long texts such as novels.
FlawedFictions is a good extension to prior long-context evals like &lt;a href="https://novelchallenge.github.io/"&gt;NoCha&lt;/a&gt;, which uses single claim verification from long texts rather than more detailed dependencies.&lt;/p&gt;
&lt;h2 id="item-response-theory-smooths-benchmarks"&gt;&lt;a class="toclink" href="#item-response-theory-smooths-benchmarks"&gt;Item response theory smooths benchmarks&lt;/a&gt;&lt;a class="header-link" href="#item-response-theory-smooths-benchmarks" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Finally, &lt;a href="https://arxiv.org/abs/2509.11106"&gt;Fluid Language Model Benchmarking&lt;/a&gt; from Valentin Hoffman (Ai2) is awesome. They use item response theory (IRT) to dynamically select samples to update a latent capability estimate of a model (in a nutshell, at step &lt;em&gt;n&lt;/em&gt;, if we think it's smart, pick a harder example, else pick easy). This gives lower-variance performance estimates which turn otherwise noisy evals into useful signals of training progress. (A model which gets 30% performance by randomly guessing noisy questions right vs one that gets it from answering hard ones is better)&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.datocms-assets.com/64837/1757692010-variance_example_blog-1.png?fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550"/&gt;&lt;/p&gt;
&lt;p&gt;I plan to talk about this paper more in the future.&lt;/p&gt;
&lt;h1 id="modifying-and-improving-models"&gt;&lt;a class="toclink" href="#modifying-and-improving-models"&gt;Modifying and improving models&lt;/a&gt;&lt;a class="header-link" href="#modifying-and-improving-models" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="data-augmentation-for-improving-long-context-models"&gt;&lt;a class="toclink" href="#data-augmentation-for-improving-long-context-models"&gt;Data augmentation for improving long-context models&lt;/a&gt;&lt;a class="header-link" href="#data-augmentation-for-improving-long-context-models" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Given the complaints I have about long-context LMs, I was pleased to see
&lt;a href="https://arxiv.org/abs/2502.14854"&gt;CLIPPER&lt;/a&gt; from Chau Minh Pham (UMD) et al, 
a simple data augmentation technique based on compression to fine-tune open models to be better long-context readers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://github.com/chtmp223/CLIPPER/blob/main/assets/img/pipeline.jpg?raw=true"/&gt;&lt;/p&gt;
&lt;p&gt;Using an chapter-by-chapter outlines and book-level outlines they use LLMs to generate True and False claim pairs. This "compression"-based approach is then verified at small scale by human annotators, to find that 83% of generated claims are completely error-free, an absolute improvement of 66% over the prior SOTA in this form of data generation. While it would be inappropriate to use such noisy data for evaluation, for training it's much better than nothing, and 34x cheaper per sample than human annotators (used to make NoCha).&lt;/p&gt;
&lt;p&gt;Using this augmented data they construct an SFT training set to fine-tune open Qwen, Llama, and ProLong models. This process doubles the performance of the tested models on the hardest NoChat dataset, and improves performance on NarrativeQA, MuSR, and their test set considerably. Although prior results showed that short-context claim verification data does also improve long-context, it probably shouldn't be a surprise that their long-context data improved model performance more.&lt;/p&gt;
&lt;p&gt;That being said, at the poster Chau told me she was a bit surprised that their data didn't improve performance more. I wonder if more denoising and bumping up that 83% correct claim rate would help here?&lt;/p&gt;
&lt;h2 id="reducing-miscalibration-in-bidirectional-knowledge"&gt;&lt;a class="toclink" href="#reducing-miscalibration-in-bidirectional-knowledge"&gt;Reducing miscalibration in bidirectional knowledge&lt;/a&gt;&lt;a class="header-link" href="#reducing-miscalibration-in-bidirectional-knowledge" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2504.11381"&gt;RankAlign&lt;/a&gt; from Juan Diego Rodriguez and Wenxuan Ding (UT Austin) was another neat paper on improving models that does something refreshingly simple to address the "generator-validator gap" (GV gap) in LMs.&lt;/p&gt;
&lt;p&gt;The GV gap refers to an LM failing to bidirectionally reflect knowledge---for example, generating &lt;code&gt;Olives are a type of fruit&lt;/code&gt;, but answering &lt;code&gt;no&lt;/code&gt; to &lt;code&gt;are olives a type of fruit?&lt;/code&gt;.
They formalize the GV gap using logprobs. Given the logprobs of a word's category membership and the logprobs of the category answer given the "are a type of" prompt, calibration represents a better correlation. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_002.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;Motivated from this framing they present a simple remedy: pairwise contrastive loss. Look at two (word, category) pairs. If the correlation relation is backwards, push them the other way!&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_003.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;This method improves performance on a few QA tasks while considerably improving the GV gap issue.&lt;/p&gt;
&lt;h1 id="personas"&gt;&lt;a class="toclink" href="#personas"&gt;Personas&lt;/a&gt;&lt;a class="header-link" href="#personas" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;I used to be really incredulous about using LMs to simulate people, but I now think techniques in this space are both promising for &lt;a href="https://openreview.net/forum?id=bttKwCZDkm"&gt;metrology&lt;/a&gt; but also present peril. I want to highlight three papers in this space presenting the feasibility and risks of realistic, consistent personas.&lt;/p&gt;
&lt;h2 id="finetuned-lms-can-successfully-impersonate-you-and-trick-your-friends"&gt;&lt;a class="toclink" href="#finetuned-lms-can-successfully-impersonate-you-and-trick-your-friends"&gt;Finetuned LMs can successfully impersonate you and trick your friends&lt;/a&gt;&lt;a class="header-link" href="#finetuned-lms-can-successfully-impersonate-you-and-trick-your-friends" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2504.04332"&gt;IMPersona&lt;/a&gt; from Ben Shi (Princeton) was an awesome and &lt;em&gt;scary&lt;/em&gt; paper. They trained LMs on &lt;em&gt;real chat logs&lt;/em&gt; submitted by participants in order to impersonate them, and then tested these personally finetuned models on a &lt;em&gt;friends and family turing test&lt;/em&gt; where additional participants who &lt;em&gt;actually know&lt;/em&gt; the submitting participants had to guess if the bot was their friend or not.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_004.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;Shockingly, these finetuned Llama models &lt;em&gt;passed&lt;/em&gt; this test 44% of the time! &lt;strong&gt;A personalized LMs could trick your friends&lt;/strong&gt;! Is all hope lost? Well, they checked for a few relationship variables which may predict whether an LM could impersonate you to someone else.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_005.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;The best protective attributes are texting frequency and AI experience. It does not matter how "close" you are to the person, but how often you text, and how experienced they are with AI. As the presenter put it to me, "grandparents are the most likely to get fooled." You better start texting grandma more!&lt;/p&gt;
&lt;h2 id="detailed-life-stories-give-more-diverse-and-distributionally-accurate-simulated-humans"&gt;&lt;a class="toclink" href="#detailed-life-stories-give-more-diverse-and-distributionally-accurate-simulated-humans"&gt;Detailed life stories give more diverse and distributionally accurate simulated humans&lt;/a&gt;&lt;a class="header-link" href="#detailed-life-stories-give-more-diverse-and-distributionally-accurate-simulated-humans" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;a href="https://arxiv.org/abs/2504.11673"&gt;Deep Binding of Language Model Virtual Personas&lt;/a&gt; Minwoo Kang and Suhong Moon (UCB) the authors tackled the problem of mode collapse in simulating political poll participants (note: I really hate this application for LMs; we should be doing social science by talking to humans. But, I see other important applications for the techniques presented here.)&lt;/p&gt;
&lt;p&gt;Rather than eliciting a Republican persona by simply prompting an LM with &lt;code&gt;You are a Republican, now answer this question&lt;/code&gt;, they provide instead produce lengthy, multi-turn interviews with the elicited persona to serve as context. From real-world questions for multiturn interviews (the &lt;a href="https://americanvoicesproject.org/"&gt;American Voices Project&lt;/a&gt;) they prompt a &lt;em&gt;base&lt;/em&gt; LM (see "Base Models Beat Aligned Models at Randomness and Creativity" below for why this is important) to generate answers to each question, and then use an RLHF-tuned critic model to perform rejection sampling, removing low-quality answers.&lt;/p&gt;
&lt;p&gt;These create rich and detailed conditioning prompts to generate more diverse personas conditioned on brief group identities. Then, they test if the distribution of persona opinions matches the distribution of real Democrat and Republican opinions including cross-group hostility and in-group vs out-group meta-perception (ie., &lt;code&gt;As a Democrat I think...&lt;/code&gt; vs &lt;code&gt;...I think Republicans think...&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_006.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;There is a scaling relationship here. As they sample more personas, the distributional gap between real human responses and LM responses closes (decreasing Wasserstein distance)&lt;/p&gt;
&lt;h2 id="personas-for-evaluating-personalization"&gt;&lt;a class="toclink" href="#personas-for-evaluating-personalization"&gt;Personas for evaluating personalization&lt;/a&gt;&lt;a class="header-link" href="#personas-for-evaluating-personalization" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the first paper, we saw a bit about how (specific) human assessment can evaluate persona adoption in LMs, and in the second we automated the process of distributional alignment of personas. However, both of these assessed fixed persona adoption. What about cases where we want the persona to dynamically evolve over time?&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2508.01674"&gt;CUPID&lt;/a&gt; from Tae Soo Kim (KAIST) fits in to the broader literature on personalized alignment to introduce a dataset for the purpose of evaluating LM's abilities to infer and use &lt;em&gt;contextual&lt;/em&gt; user  preferences which evolve over time using simulated user interactions.&lt;/p&gt;
&lt;p&gt;They produce personas expressed as a set of dialogue sessions: &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;(c_i, p_i, D_i)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span aria-hidden="true" class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut" style="height:1em;vertical-align:-0.25em;"&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;c&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height:0.3117em;"&gt;&lt;span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"&gt;&lt;span class="pstrut" style="height:2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height:0.15em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mspace" style="margin-right:0.1667em;"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;p&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height:0.3117em;"&gt;&lt;span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"&gt;&lt;span class="pstrut" style="height:2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height:0.15em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mspace" style="margin-right:0.1667em;"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal" style="margin-right:0.02778em;"&gt;D&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height:0.3117em;"&gt;&lt;span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"&gt;&lt;span class="pstrut" style="height:2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height:0.15em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; triples of some "context factor," "contextual preference," and a multi-turn dialogue which expresses the preference. The context factor is some literal string within the text, such as "Canon MP-E 65mm Macro Lens," which showing up in a session would reflect a contextual preference for exact specific details regarding equipment for a photographer persona. They generate these using detailed persona descriptions and LMs, with human verification.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://saxon.me/blog/2025/colm2025/images/IMG_Dispatch from COLM_007.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;They then use these dialogues for two tasks, contextual preference &lt;em&gt;inference&lt;/em&gt; and preference-conditioned &lt;em&gt;generation&lt;/em&gt;.
Checking if the preference is satisfied is non-trivial... how do you verify that "sufficient technical specificity" is present in a generated passage automatically? To do this they generate atomic checklists of requirements for each preference and check them with LM-as-a-judge.&lt;/p&gt;
&lt;p&gt;They find that both inferring these preferences and generating answers with them is quite hard for frontier models. This was a cool application of LM-generated personas, and I think it has some "design patterns" around the character stories and rubrics that may be applied in my own work.&lt;/p&gt;
&lt;p&gt;I have a lot more thoughts on personas, namely &lt;em&gt;what even is a persona&lt;/em&gt; which I hope to eventually get around to writing. Maybe stay tuned? Pester me with your thoughts about this question!&lt;/p&gt;
&lt;h1 id="roses-and-thorns"&gt;&lt;a class="toclink" href="#roses-and-thorns"&gt;Roses and thorns&lt;/a&gt;&lt;a class="header-link" href="#roses-and-thorns" title="Permalink to this section"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Overall, I had a blast at COLM 2025, and I think it was just about as great as the first one. 
There was Great mix of people and ideas: it felt like the "NLP tracks" at NeurIPS, without all the AI bros.
In addition to having a splendid crowd (go figure, I'd love being around my friends who work in my area), I really think the structure of the venue itself makes fruitful interactions more likely.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Having a single track is huge.&lt;/strong&gt; For the entire duration of the conference, all participating attendees are in roughly the same area, or the same chokepoints around and near it. Even though I mainly participate in the hallway track at conferences, I could reliably run into people to have fun conversations with in the same few places.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The poster sessions were long&lt;/strong&gt;. More conferences should do this. Poster sessions are by far the most fruitful site for networking and idea exchange, and the two hour-long sessions gave me plenty of time to make my way through and take a real glance at &lt;em&gt;all the posters&lt;/em&gt;. Not only is 2h a long time on an absolute level, at COLM the &lt;em&gt;time per poster&lt;/em&gt; was quite high as well.&lt;/p&gt;
&lt;p&gt;Unfortunately both of these pros are artifacts of a small venue. As COLM grows it may become hard to sustain this high-participation setup.
I think in order to preserve this character we need to &lt;strong&gt;not run orals and poster sessions simultaneously&lt;/strong&gt;. That way even as the number of parallel poster tracks grows, they at least won't be competing for attention with orals, and &lt;strong&gt;all participants will be in the same vicinity&lt;/strong&gt;.
Also, I think the overall ratio of time dedicated to posters needs to be maintained, or even expanded.&lt;/p&gt;
&lt;p&gt;It was super fun to present our reasoning model work (&lt;a href="https://arxiv.org/abs/2504.13367"&gt;ThoughtTerminator&lt;/a&gt;) which is probably the most zeitgeisty conference presentation I have ever given. I definitely got more interactions out of it and didn't feel a need to hard sell the value of my work as much as I have in the past.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A polaroid Xiao took of me after we presented our paper." src="https://saxon.me/blog/2025/colm2025/images/IMG_0145.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;As of now, unlike NeurIPS, there is not that much involvement of big lab or AI safety folks yet, which feels like a double-edged sword. Though that work isn't my cup of tea, it's very relevant, and I'd like to be around some of it!&lt;/p&gt;
&lt;p&gt;Given the amount of positive attention the conference is getting, I do have some concerns.
I am a little worried that it has been so great so far because it is small and &lt;em&gt;not a prestige target&lt;/em&gt;.
Nobody who is optimizing for bean counters is focusing on COLM yet, so the reviewing process isn't overwhelmed, and really dynamic, interested people are the only ones showing up.
Will that change as it grows?
Also, because it isn't yet "the arena" it isn't overrun by AI bros. Will that change as it grows?&lt;/p&gt;
&lt;p&gt;Also, as much as I love Canada, I think putting top ML conferences there is getting a lil played out.
I have visited the big 3 cities of Canada in a shorter period of time than I have the US, thanks solely to conferences.
As a certified West Coast bro, I have literally spent more time in the big 3 Canadian cities than I have New York City.
Considering the visa issues that people are having with Canada (and the US) can we go better than holding conferences there?&lt;/p&gt;
&lt;p&gt;I don't want to end on a negative note, so I will reiterate: &lt;strong&gt;COLM was amazing and I will continue to prioritize it&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;(see &lt;a href="https://techcrunch.com/2024/06/29/geminis-data-analyzing-abilities-arent-as-good-as-google-claims/"&gt;TechCrunch interview&lt;/a&gt; with Marzena Karpinska and myself on this issue) &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Research"></category><category term="colm"></category><category term="conference"></category></entry></feed>